{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# os.environ['SPARK_HOME'] = \"/Users/rutvik/Study/DBMS/spark-3.4.2-bin-hadoop3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n",
      "24/04/12 16:10:47 WARN Utils: Your hostname, Rutvik.local resolves to a loopback address: 127.0.0.1; using 10.0.0.46 instead (on interface en0)\n",
      "24/04/12 16:10:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/rutvik/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/rutvik/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-acd5b9af-9b86-4b84-8c06-9230bf5e014e;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      ":: resolution report :: resolve 98ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-acd5b9af-9b86-4b84-8c06-9230bf5e014e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/2ms)\n",
      "24/04/12 16:10:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"pyspark from public s3\")\\\n",
    "       .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\")\\\n",
    "       .config(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\") \\\n",
    "       .getOrCreate()\n",
    "sqlContext = SparkSession(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 16:10:50 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"s3a://multi-token-completion/human_annotated/labeled_completions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"public_s3_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+--------------------+--------+-------+----------------+--------------------+-------+\n",
      "|_c0|         masked_sent|    original_phrase|suggested_completion|is_valid|is_fact|is_more_abstract|is_factually_correct| method|\n",
      "+---+--------------------+-------------------+--------------------+--------+-------+----------------+--------------------+-------+\n",
      "|  0|Line B was intend...|the State of Mexico|        buenos aires|       1|      1|             0.0|                 0.0|  t5-3b|\n",
      "|  1|Line B was intend...|the State of Mexico|      Rio de Janeiro|       1|      1|             0.0|                 0.0|    ilm|\n",
      "|  2|Line B was intend...|the State of Mexico|              Cuenca|       1|      1|             0.0|                 0.0|   EMAT|\n",
      "|  3|Line B was intend...|the State of Mexico|            the u.s.|       1|      1|             1.0|                 0.0|t5-base|\n",
      "|  4|Lloyd Richards, s...|             Raisin|                 day|       1|      1|             0.0|                 0.0|t5-base|\n",
      "|  5|Lloyd Richards, s...|             Raisin|              guitar|       1|      1|             0.0|                 0.0|    ilm|\n",
      "|  6|Lloyd Richards, s...|             Raisin|              Raisin|       1|      1|             0.0|                 1.0|   EMAT|\n",
      "|  7|Lloyd Richards, s...|             Raisin|              raisin|       1|      1|             0.0|                 1.0|  t5-3b|\n",
      "|  8|Ludlow's populati...|      Interstate 90| the ludlow turnpike|       1|      1|             0.0|                 0.0|t5-base|\n",
      "|  9|Ludlow's populati...|      Interstate 90|       interstate 93|       1|      1|             0.0|                 0.0|  t5-3b|\n",
      "+---+--------------------+-------------------+--------------------+--------+-------+----------------+--------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 16:11:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , masked_sent, original_phrase, suggested_completion, is_valid, is_fact, is_more_abstract, is_factually_correct, method\n",
      " Schema: _c0, masked_sent, original_phrase, suggested_completion, is_valid, is_fact, is_more_abstract, is_factually_correct, method\n",
      "Expected: _c0 but found: \n",
      "CSV file: s3a://multi-token-completion/human_annotated/labeled_completions.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from public_s3_bucket\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_camp_deets = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"path\", \"./data/Health_Camp_Detail.csv\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Health_Camp_ID: integer (nullable = true)\n",
      " |-- Camp_Start_Date: string (nullable = true)\n",
      " |-- Camp_End_Date: string (nullable = true)\n",
      " |-- Category1: string (nullable = true)\n",
      " |-- Category2: string (nullable = true)\n",
      " |-- Category3: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "health_camp_deets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|Health_Camp_ID|Camp_Start_date|\n",
      "+--------------+---------------+\n",
      "|          6560|      16-Aug-03|\n",
      "|          6530|      16-Aug-03|\n",
      "|          6544|      03-Nov-03|\n",
      "|          6585|      22-Nov-03|\n",
      "|          6561|      30-Nov-03|\n",
      "|          6581|      07-Dec-03|\n",
      "|          6564|      17-Dec-03|\n",
      "|          6557|      04-Jan-04|\n",
      "|          6538|      09-Jan-04|\n",
      "|          6546|      09-Jan-04|\n",
      "+--------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "health_camp_deets.select(\"Health_Camp_ID\", \"Camp_Start_date\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_camp_deets.createOrReplaceTempView(\"health_camp_deets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-------------+---------+---------+---------+\n",
      "|Health_Camp_ID|Camp_Start_Date|Camp_End_Date|Category1|Category2|Category3|\n",
      "+--------------+---------------+-------------+---------+---------+---------+\n",
      "|          6560|      16-Aug-03|    20-Aug-03|    First|        B|        2|\n",
      "|          6530|      16-Aug-03|    28-Oct-03|    First|        C|        2|\n",
      "|          6544|      03-Nov-03|    15-Nov-03|    First|        F|        1|\n",
      "|          6585|      22-Nov-03|    05-Dec-03|    First|        E|        2|\n",
      "|          6561|      30-Nov-03|    18-Dec-03|    First|        E|        1|\n",
      "|          6581|      07-Dec-03|    13-Jun-04|    First|        F|        2|\n",
      "|          6564|      17-Dec-03|    11-Jun-04|    First|        C|        2|\n",
      "|          6557|      04-Jan-04|    09-Jan-04|    First|        C|        2|\n",
      "|          6538|      09-Jan-04|    04-Feb-05|    First|        F|        2|\n",
      "|          6546|      09-Jan-04|    17-Jan-04|    First|        E|        2|\n",
      "+--------------+---------------+-------------+---------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from health_camp_deets\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
